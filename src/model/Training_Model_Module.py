# ========================== Core Utilities ==========================
import os
import re
import math
import random
import warnings
from datetime import datetime
from itertools import combinations
from collections import Counter
from typing import List, Tuple, Dict, Optional

# ========================== Scientific Computing ==========================
import numpy as np
import pandas as pd
import scipy
from scipy.stats import skew

# ========================== Visualization ==========================
import matplotlib.pyplot as plt
import seaborn as sns
import missingno
import mplcursors
from matplotlib.colors import LinearSegmentedColormap

# ========================== Progress Tracking ==========================
from tqdm.notebook import tqdm
tqdm.pandas()

# ========================== Preprocessing ==========================
from sklearn.decomposition import PCA, TruncatedSVD
from sklearn.feature_selection import SelectFromModel
from sklearn.impute import SimpleImputer
from sklearn.manifold import TSNE
from sklearn.preprocessing import (
    LabelEncoder, MinMaxScaler, OneHotEncoder,
    OrdinalEncoder, StandardScaler
)

# ========================== Model Selection & Evaluation ==========================
from sklearn.model_selection import (
    GridSearchCV, KFold, ParameterGrid, RandomizedSearchCV,
    StratifiedKFold, cross_val_predict, cross_val_score,
    train_test_split
)

from sklearn.metrics import (
    accuracy_score, auc, classification_report, confusion_matrix,
    f1_score, make_scorer, precision_recall_curve,
    precision_score, recall_score, roc_auc_score, roc_curve
)
from imblearn.metrics import classification_report_imbalanced

# ========================== Pipelines ==========================
from sklearn.pipeline import make_pipeline
from imblearn.pipeline import Pipeline, make_pipeline as imbalanced_make_pipeline

# ========================== Imbalanced Data Handling ==========================
from imblearn.over_sampling import SMOTE, RandomOverSampler
from imblearn.under_sampling import NearMiss, RandomUnderSampler

# ========================== Scikit-learn Models ==========================
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.svm import LinearSVC, SVC
from sklearn.tree import DecisionTreeClassifier

# ========================== Gradient Boosting Models ==========================
import xgboost as xgb
from xgboost import XGBClassifier

import lightgbm as lgb
from lightgbm import LGBMClassifier

from catboost import CatBoostClassifier, CatBoostRegressor, Pool, cv

def train_dev_test (X, y):
    """
    Chia dá»¯ liá»‡u Ä‘áº§u vÃ o thÃ nh 3 táº­p: train, dev (validation) vÃ  test, vá»›i tá»‰ lá»‡ 60/20/20.

    Tham sá»‘:
        df (pd.DataFrame): DataFrame Ä‘áº§u vÃ o, báº¯t buá»™c pháº£i cÃ³ cá»™t 'label' lÃ  nhÃ£n phÃ¢n loáº¡i.

    Tráº£ vá»:
        X_train (pd.DataFrame): Äáº·c trÆ°ng cho táº­p huáº¥n luyá»‡n.
        y_train (pd.Series): NhÃ£n cho táº­p huáº¥n luyá»‡n.
        X_dev (pd.DataFrame): Äáº·c trÆ°ng cho táº­p validation (dev).
        y_dev (pd.Series): NhÃ£n cho táº­p validation.
        X_test (pd.DataFrame): Äáº·c trÆ°ng cho táº­p kiá»ƒm tra (test).
        y_test (pd.Series): NhÃ£n cho táº­p kiá»ƒm tra.

    Ghi chÃº:
        - TÃ¡ch theo tá»‰ lá»‡: Train (60%), Dev (20%), Test (20%).
        - Dá»¯ liá»‡u Ä‘Æ°á»£c **stratify** theo nhÃ£n Ä‘á»ƒ Ä‘áº£m báº£o phÃ¢n phá»‘i lá»›p Ä‘á»“ng Ä‘á»u giá»¯a cÃ¡c táº­p.
        - In ra sá»‘ lÆ°á»£ng má»—i lá»›p trong tá»«ng táº­p Ä‘á»ƒ kiá»ƒm tra cÃ¢n báº±ng.
    """
    X_trainval, X_test, y_trainval, y_test = train_test_split(
        X, y, test_size=0.2, stratify=y, random_state=42, shuffle=True)

    X_train, X_dev, y_train, y_dev = train_test_split(
        X_trainval, y_trainval, test_size=0.25, stratify=y_trainval, random_state=42, shuffle=True)

    print("Train:", Counter(y_train))
    print("Dev:", Counter(y_dev))
    print("Test:", Counter(y_test))
    
    return X_train, y_train, X_dev, y_dev, X_test, y_test

def evaluate_model(y_true, y_pred, y_proba=None, dataset_name='', target_names=None):
    """
    ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh phÃ¢n loáº¡i Ä‘a lá»›p hoáº·c nhá»‹ phÃ¢n.

    Tham sá»‘:
        y_true: NhÃ£n tháº­t
        y_pred: NhÃ£n dá»± Ä‘oÃ¡n
        y_proba: XÃ¡c suáº¥t (náº¿u cÃ³) - khÃ´ng dÃ¹ng cho Ä‘a lá»›p máº·c Ä‘á»‹nh
        dataset_name: TÃªn táº­p dá»¯ liá»‡u (hiá»ƒn thá»‹)
        target_names: Danh sÃ¡ch tÃªn lá»›p ['NEG', 'NEU', 'POS'] Ä‘á»ƒ hiá»ƒn thá»‹ confusion matrix

    """

    print(f"\nðŸ” Evaluation on {dataset_name} set:")

    # CÃ¡c chá»‰ sá»‘ macro (Ä‘a lá»›p)
    print("Accuracy :", round(accuracy_score(y_true, y_pred), 4))
    print("Precision (macro):", round(precision_score(y_true, y_pred, average='macro'), 4))
    print("Recall (macro)   :", round(recall_score(y_true, y_pred, average='macro'), 4))
    print("F1-score (macro) :", round(f1_score(y_true, y_pred, average='macro'), 4))

    # ROC AUC chá»‰ phÃ¹ há»£p vá»›i nhá»‹ phÃ¢n hoáº·c xá»­ lÃ½ Ä‘áº·c biá»‡t â†’ nÃªn táº¡m bá» trong Ä‘a lá»›p

    # BÃ¡o cÃ¡o chi tiáº¿t
    print("\nðŸ“‹ Classification Report:")
    print(classification_report(y_true, y_pred, target_names=target_names))

    # Confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(5.5, 4.5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges',
                xticklabels=target_names,
                yticklabels=target_names)
    plt.title(f'Confusion Matrix - {dataset_name}')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.tight_layout()
    plt.show()

    
def optimize_threshold_by_f1(model, X_test, y_test, start=0.05, end=0.95, step=0.01, plot=True):
    """
    Tá»‘i Æ°u ngÆ°á»¡ng phÃ¢n loáº¡i Ä‘á»ƒ Ä‘áº¡t F1-score cao nháº¥t.

    Parameters:
        model: MÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n, pháº£i cÃ³ phÆ°Æ¡ng thá»©c `predict_proba`.
        X_test (DataFrame or array): Dá»¯ liá»‡u kiá»ƒm tra.
        y_test (Series or array): NhÃ£n tháº­t.
        start (float): NgÆ°á»¡ng báº¯t Ä‘áº§u thá»­.
        end (float): NgÆ°á»¡ng káº¿t thÃºc thá»­.
        step (float): BÆ°á»›c nháº£y cá»§a ngÆ°á»¡ng.
        plot (bool): CÃ³ hiá»ƒn thá»‹ biá»ƒu Ä‘á»“ khÃ´ng.

    Returns:
        best_thresh (float): NgÆ°á»¡ng phÃ¢n loáº¡i tá»‘t nháº¥t.
        best_f1 (float): F1-score cao nháº¥t tÆ°Æ¡ng á»©ng.
    """
    # Láº¥y xÃ¡c suáº¥t cá»§a lá»›p 1
    y_proba = model.predict_proba(X_test)[:, 1]

    thresholds = np.arange(start, end, step)
    f1_scores = []

    for thresh in thresholds:
        y_pred = (y_proba >= thresh).astype(int)
        f1 = f1_score(y_test, y_pred)
        f1_scores.append(f1)

    best_idx = np.argmax(f1_scores)
    best_thresh = thresholds[best_idx]
    best_f1 = f1_scores[best_idx]

    print(f"âœ… Best threshold: {best_thresh:.2f} with F1-score: {best_f1:.4f}")

    if plot:
        plt.figure(figsize=(10,6))
        plt.plot(thresholds, f1_scores, marker='o')
        plt.xlabel("Threshold")
        plt.ylabel("F1-score")
        plt.title("Tá»‘i Æ°u threshold phÃ¢n loáº¡i")
        plt.grid(True)
        plt.axvline(best_thresh, color='red', linestyle='--', label=f'Best: {best_thresh:.2f}')
        plt.legend()
        plt.tight_layout()
        plt.show()

    return best_thresh, best_f1


def tune_logistic_regression_gridsearch(
    X: pd.DataFrame,
    y: pd.Series,
    scoring: str = "f1",
    cv_folds: int = 5,
    random_state: int = 42
) -> Tuple[Dict, pd.DataFrame]:
    """
    Tune Logistic Regression hyperparameters sá»­ dá»¥ng GridSearchCV.
    """

    # Grid nhá» gá»n
    param_grid = {
        "penalty": ["l1", "l2"],
        "C": [0.01, 0.1, 1, 10],
        "solver": ["liblinear"],  # Ä‘áº£m báº£o há»— trá»£ cáº£ l1 vÃ  l2
    }

    model = LogisticRegression(max_iter=1000, random_state=random_state)

    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)

    scorer = make_scorer(f1_score, average="binary" if y.nunique() == 2 else "macro")

    grid = GridSearchCV(
        estimator=model,
        param_grid=param_grid,
        scoring=scorer,
        cv=cv,
        n_jobs=-1,
        verbose=0,
        return_train_score=True
    )

    grid.fit(X, y)

    best_params = grid.best_params_
    results_df = pd.DataFrame(grid.cv_results_)

    return best_params, results_df

def tune_lightgbm_gridsearch(
    X: pd.DataFrame,
    y: pd.Series,
    scoring: str = "f1",
    cv_folds: int = 5,
    random_state: int = 42
) -> Tuple[Dict, pd.DataFrame]:
    """
    Tune LightGBM hyperparameters sá»­ dá»¥ng GridSearchCV.
    """

    # Grid Ä‘Æ¡n giáº£n, gá»n nháº¹
    param_grid = {
        'num_leaves': [15, 31, 63],
        'learning_rate': [0.01, 0.1],
        'n_estimators': [100, 200],
        'max_depth': [-1, 5, 10]
    }

    model = LGBMClassifier(verbose=-1, random_state=random_state, n_jobs=-1)

    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)

    scorer = make_scorer(f1_score, average="binary" if y.nunique() == 2 else "macro")

    grid = GridSearchCV(
        estimator=model,
        param_grid=param_grid,
        scoring=scorer,
        cv=cv,
        n_jobs=-1,
        verbose=0,
        return_train_score=True
    )

    grid.fit(X, y)

    best_params = grid.best_params_
    results_df = pd.DataFrame(grid.cv_results_)

    return best_params, results_df

def tune_gaussian_nb_gridsearch(
    X: pd.DataFrame,
    y: pd.Series,
    scoring: str = "f1",
    cv_folds: int = 5
) -> Tuple[Dict, pd.DataFrame]:
    """
    Tune GaussianNB hyperparameter (var_smoothing) sá»­ dá»¥ng GridSearchCV.
    
    Returns:
        best_params: dict of best parameters
        results_df: DataFrame of GridSearchCV results
    """
    param_grid = {
        "var_smoothing": [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]
    }

    model = GaussianNB()

    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
    scorer = make_scorer(f1_score, average="binary" if y.nunique() == 2 else "macro")

    grid = GridSearchCV(
        estimator=model,
        param_grid=param_grid,
        scoring=scorer,
        cv=cv,
        n_jobs=-1,
        verbose=0,
        return_train_score=True
    )

    grid.fit(X, y)

    best_params = grid.best_params_
    results_df = pd.DataFrame(grid.cv_results_)

    return best_params, results_df

def tune_xgboost_gridsearch(
    X: pd.DataFrame,
    y: pd.Series,
    scoring: str = "f1",
    cv_folds: int = 5,
    random_state: int = 42
) -> Tuple[Dict, pd.DataFrame]:
    """
    Tune XGBoost hyperparameters sá»­ dá»¥ng GridSearchCV.
    
    Tráº£ vá»:
        best_params: dict of best parameters
        results_df: DataFrame of GridSearchCV results
    """

    # Grid nhá» gá»n, hiá»‡u quáº£
    param_grid = {
        'n_estimators': [100, 200],
        'learning_rate': [0.01, 0.1],
        'max_depth': [3, 6, 10],
        'subsample': [0.8, 1.0],
        'colsample_bytree': [0.8, 1.0],
    }

    model = XGBClassifier(
        objective='binary:logistic' if y.nunique() == 2 else 'multi:softprob',
        use_label_encoder=False,
        eval_metric='logloss',
        random_state=random_state,
        n_jobs=-1,
        verbosity=0
    )

    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)
    scorer = make_scorer(f1_score, average="binary" if y.nunique() == 2 else "macro")

    grid = GridSearchCV(
        estimator=model,
        param_grid=param_grid,
        scoring=scorer,
        cv=cv,
        n_jobs=-1,
        verbose=0,
        return_train_score=True
    )

    grid.fit(X, y)

    best_params = grid.best_params_
    results_df = pd.DataFrame(grid.cv_results_)

    return best_params, results_df

def tune_random_forest_gridsearch(
    X: pd.DataFrame,
    y: pd.Series,
    scoring: str = "f1",
    cv_folds: int = 5,
    random_state: int = 42
) -> Tuple[Dict, pd.DataFrame]:
    """
    Tune Random Forest hyperparameters sá»­ dá»¥ng GridSearchCV/
    
    Tráº£ vá»:
        best_params: dict of best parameters
        results_df: DataFrame of GridSearchCV results
    """
    param_grid = {
        'n_estimators': [100],                 # Giá»¯ 1 giÃ¡ trá»‹ phá»• biáº¿n
        'max_depth': [None, 10],               # KhÃ´ng giá»›i háº¡n hoáº·c vá»«a pháº£i
        'min_samples_split': [2],              # Máº·c Ä‘á»‹nh
        'min_samples_leaf': [1],               # Máº·c Ä‘á»‹nh
        'max_features': ['sqrt']               # Hiá»‡u quáº£ vá»›i cÃ¢y
    }

    model = RandomForestClassifier(random_state=random_state, n_jobs=-1)

    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)
    scorer = make_scorer(f1_score, average="binary" if y.nunique() == 2 else "macro")

    grid = GridSearchCV(
        estimator=model,
        param_grid=param_grid,
        scoring=scorer,
        cv=cv,
        n_jobs=-1,
        verbose=0,
        return_train_score=True
    )

    grid.fit(X, y)

    best_params = grid.best_params_
    results_df = pd.DataFrame(grid.cv_results_)

    return best_params, results_df

def tune_svm_gridsearch(
    X: pd.DataFrame,
    y: pd.Series,
    scoring: str = "f1",
    cv_folds: int = 5,
    random_state: int = 42
) -> Tuple[Dict, pd.DataFrame]:
    param_grid = [
        {'kernel': ['linear'], 'C': [0.1, 1, 10]},
        {'kernel': ['rbf'], 'C': [0.1, 1, 10], 'gamma': ['scale', 'auto']}
    ]

    model = SVC(random_state=random_state)

    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)
    scorer = make_scorer(f1_score, average="binary" if y.nunique() == 2 else "macro")

    grid = GridSearchCV(
        estimator=model,
        param_grid=param_grid,
        scoring=scorer,
        cv=cv,
        n_jobs=-1,
        verbose=0,
        return_train_score=True
    )

    grid.fit(X, y)

    best_params = grid.best_params_
    results_df = pd.DataFrame(grid.cv_results_)

    return best_params, results_df


def tune_knn_gridsearch(
    X: pd.DataFrame,
    y: pd.Series,
    scoring: str = "f1",
    cv_folds: int = 5,
    random_state: int = 42
) -> Tuple[Dict, pd.DataFrame]:
    param_grid = {
        'n_neighbors': [3, 5, 7, 9],
        'weights': ['uniform', 'distance'],
        'metric': ['euclidean', 'manhattan']
    }

    model = KNeighborsClassifier()

    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)
    scorer = make_scorer(f1_score, average="binary" if y.nunique() == 2 else "macro")

    grid = GridSearchCV(
        estimator=model,
        param_grid=param_grid,
        scoring=scorer,
        cv=cv,
        n_jobs=-1,
        verbose=0,
        return_train_score=True
    )

    grid.fit(X, y)

    best_params = grid.best_params_
    results_df = pd.DataFrame(grid.cv_results_)

    return best_params, results_df


def tune_catboost_gridsearch(
    X: pd.DataFrame,
    y: pd.Series,
    scoring: str = "f1",
    cv_folds: int = 5,
    random_state: int = 42
) -> Tuple[Dict, pd.DataFrame]:
    param_grid = {
        'iterations': [100, 300],
        'learning_rate': [0.01, 0.1],
        'depth': [4, 6, 8]
    }

    model = CatBoostClassifier(
        verbose=0,                 # KhÃ´ng in log trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n
        random_state=random_state,
        task_type='CPU'           # Äáº£m báº£o tÆ°Æ¡ng thÃ­ch khÃ´ng cáº§n GPU
    )

    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)
    scorer = make_scorer(f1_score, average="binary" if y.nunique() == 2 else "macro")

    grid = GridSearchCV(
        estimator=model,
        param_grid=param_grid,
        scoring=scorer,
        cv=cv,
        n_jobs=-1,
        verbose=0,
        return_train_score=True
    )

    grid.fit(X, y)

    best_params = grid.best_params_
    results_df = pd.DataFrame(grid.cv_results_)

    return best_params, results_df


def tune_decision_tree_gridsearch(
    X: pd.DataFrame,
    y: pd.Series,
    scoring: str = "f1",
    cv_folds: int = 5,
    random_state: int = 42
) -> Tuple[Dict, pd.DataFrame]:
    param_grid = {
        'criterion': ['gini', 'entropy'],
        'max_depth': [None, 5, 10, 20],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }

    model = DecisionTreeClassifier(random_state=random_state)

    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)
    scorer = make_scorer(f1_score, average="binary" if y.nunique() == 2 else "macro")

    grid = GridSearchCV(
        estimator=model,
        param_grid=param_grid,
        scoring=scorer,
        cv=cv,
        n_jobs=-1,
        verbose=0,
        return_train_score=True
    )

    grid.fit(X, y)

    best_params = grid.best_params_
    results_df = pd.DataFrame(grid.cv_results_)

    return best_params, results_df

def tune_mlp_gridsearch(
    X: pd.DataFrame,
    y: pd.Series,
    scoring: str = "f1",
    cv_folds: int = 5,
    random_state: int = 42
) -> Tuple[Dict, pd.DataFrame]:
    param_grid = {
        'hidden_layer_sizes': [(100,), (50, 50), (100, 50)],
        'activation': ['relu', 'tanh'],
        'solver': ['adam', 'sgd'],
        'alpha': [0.0001, 0.001],  # L2 regularization
        'learning_rate': ['constant', 'adaptive']
    }

    model = MLPClassifier(max_iter=300, random_state=random_state)

    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)
    scorer = make_scorer(f1_score, average="binary" if y.nunique() == 2 else "macro")

    grid = GridSearchCV(
        estimator=model,
        param_grid=param_grid,
        scoring=scorer,
        cv=cv,
        n_jobs=-1,
        verbose=0,
        return_train_score=True
    )

    grid.fit(X, y)

    best_params = grid.best_params_
    results_df = pd.DataFrame(grid.cv_results_)

    return best_params, results_df